<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-01-18T15:58:17-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Sam Sinai</title><subtitle>My work/thoughts related to computational biology and machine learning. 
</subtitle><entry><title type="html">Coupon collecting, viral sex, and evolution</title><link href="http://localhost:4000/jekyll/update/2018/03/29/Coupon-collecting,-viral-sex,-and-evolution.html" rel="alternate" type="text/html" title="Coupon collecting, viral sex, and evolution" /><published>2018-03-29T17:59:35-04:00</published><updated>2018-03-29T17:59:35-04:00</updated><id>http://localhost:4000/jekyll/update/2018/03/29/Coupon-collecting,-viral-sex,-and-evolution</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/03/29/Coupon-collecting,-viral-sex,-and-evolution.html">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
          });

        MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
            for(i=0; i &lt; all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
            }
        });

&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; charset=&quot;utf-8&quot; src=&quot;https://vincenttam.github.io/javascripts/MathJaxLocal.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;In this blog post I give a high level overview of &lt;a href=&quot;http://rsif.royalsocietypublishing.org/content/15/139/20180003&quot;&gt;our recently published&lt;/a&gt; work: &lt;strong&gt;Primordial sex facilitates the emergence of evolution&lt;/strong&gt; (preprint &lt;a href=&quot;https://arxiv.org/abs/1612.00825&quot;&gt;here&lt;/a&gt;). To make the model more accesible to non-biologists, I start with a famous probablity puzzle: the &lt;strong&gt;“coupon collector’s problem”&lt;/strong&gt;, add a few additional twists, and argue that this is the underlying probabilities that are interesting for certain biological processes that try to generate complexity through exchange of information. The model is general enough that could find other domains of application, whenever the coupon collector is applicable (learning through merge (like Chomsky and Papadimitriou have explored) is one area I can think of). Here skip the mathematical derivation steps, which are covered in the paper, and jump straight to the results.&lt;/p&gt;

&lt;h2 id=&quot;the-probability-puzzle&quot;&gt;The probability puzzle&lt;/h2&gt;

&lt;p&gt;A well known problem in probability is the so called &lt;a href=&quot;https://en.wikipedia.org/wiki/Coupon_collector%27s_problem&quot;&gt;“ coupon collector’s” problem&lt;/a&gt;. It goes as follows: imagine there are \(n\) unique types of coupons and you are trying to collect all of them. Each day you get to pick up a single coupon without knowing its type. The coupon you pick up may be new, or it may be of the type that you already have. The question of interest is how many days does it take for you to collect all \(n\) coupons. In the classical case, often a question in probability courses, the answer is simple to calculate:&lt;/p&gt;

\[E(T)= n. H_n \approx n \log n\]

&lt;p&gt;Here \(E(T)\) is the expected time to success and \(H_n\)  is the nth &lt;a href=&quot;https://en.wikipedia.org/wiki/Harmonic_number&quot;&gt;Harmonic number&lt;/a&gt;. Now consider the following twist, what if, you were allowed to pick up more than one coupon per day, each with independent probability p. Let’s call this variant the “Album collector’s problem”.  Each day you collect an album with some coupons in it, and add those that are new to your collection. How many days would it take for you to pick up all of the coupons?&lt;/p&gt;

&lt;p&gt;Letting \(q=1-p\), the answer is expressed as&lt;/p&gt;

\[E(T)=\sum_{i=1}^n {n \choose i}\frac{(-1)^{i}}{1-q^{-i}} \approx log_q (n)\]

&lt;p&gt;With a little effort one can see that if \(p &amp;gt; 1/n\). This is quite fast. It will take much less time to collect the coupons as compared to the “once a day” routine.&lt;/p&gt;

&lt;p&gt;This is well known in the CS community (where it’s used for radix sort and skip lists [1,2]) and in fact some famous geneticists [2] wrote a paper about how it relates to evolutionary search processes on simple fitness ladnscapes.&lt;/p&gt;

&lt;p&gt;In our recent work, we add an additional twist. What if, every night, there was a possibility that a “friend” would steal your album with everything you collected so far?&lt;/p&gt;

&lt;p&gt;If this probability \(\delta\) was very high, e.g. it was certain (\(\delta=1\)) that you lost everything every night, in order to have a full album you need exponentially many days to collect all coupons at once. This is because an independent sampling of coupons gives a \(p^n\) chance that you will hit all coupons together. But if the friends stealing habits were more sporadic, you might not be in a disastrous shape.&lt;/p&gt;

&lt;p&gt;Given p and \(\delta\),we compute the expression for this case exactly (see the article for the details). A harder task is to be able to compare this formula with the best case scenario that you never lose your album (\(\delta=0\)), and the worst case scenario that you lose everything every night.&lt;/p&gt;

&lt;p&gt;With a bunch of approximation tricks (including some heavy lifting by Jason) we manage to boil this problem down to a functional form that can be intuitively compared with our boundary cases.&lt;/p&gt;

\[E(T) \approx \frac{1}{\delta}n^{(\delta/p)}\]

&lt;p&gt;This quantifies the trade-off for collecting coupons between p and \(\delta\). Given a fixed \(\delta\) w.r.t \(n\), we are no longer in the exponential regime.&lt;/p&gt;

&lt;h2 id=&quot;the-biology&quot;&gt;The Biology&lt;/h2&gt;

&lt;p&gt;Why is this interesting? This is not just a probability puzzle. As mentioned before there are collection processes that are simular to this in biology. Early biological cells needed to collect useful stuff (genes, proteins, or other material), but could also collect fatal material in the process.  Of particular interest to me, and what inspired this calculation, is the process known as multiplicity reactivation (MR)[3]. In this process, functionally deficient viruses co-infect cells, and “cover” each other’s deficiencies, which allows them to generate fully functional virions and infect new cells. A lot of viral innovation through reassortment also follows this type of collection. Some Timoviruses are obligate cooperators: meaning they require other virions to co-infect with them to function. Of course, \(n\) there is usually small enough that these calculations don’t really save you from disasters, they simply don’t occur.&lt;/p&gt;

&lt;p&gt;There are three reasons I find this line of investingation fascinating.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Origin of life:&lt;/strong&gt; An “auto-catalytic set” is a commonly invoked concept in the origin of life (and so are “compositional genomes”) [4,5]. Most people consider the starting core of this set to be necessarily small due to probabilistic arguments of co-occurence. Because protocells can merge, we argue that the small core is not necessarily a restriction. Cells could have accumulated a large set of functions, in relatively quick manner, even if parasites, division, or death would set them back during the process.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Viral sex:&lt;/strong&gt; This computation gives hints towards what could have been possible in the viral world. Huge gene transfer and trying new combinations (like MR) could have been going on all the time. Some phages save their dying photosynthetic hosts[6] by introducing necessary genes into them. I think given the efficiency of this process, it might give a bit more plausibility to the “virus first” picture, a hypothesis on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Virus#Origins&quot;&gt;origins of viruses&lt;/a&gt; that is currently disfavored (for many good reasons, the “escape hypothesis” is dominant, but it is not mutually exclusive with virus first). The speculative idea here is that viruses with different simple capabilities might have been sexually mixing in their rather empty or dysfunctional protocells, and eventually, helped fully independent reproductive cells emerged. I.e. ancient protocells exchanged modules through virus like entities, and generated an auto-catalytic network of “reactor” protocells and “viral exchange packets” that through a network, resulted in reproduction. At some point a host cell contained all the necessary material to reproduce independently.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Fitness landscapes:&lt;/strong&gt; Per Wilf and Ewens[2], this type of thinking can be extended to fitness landscapes. In their work, they consider a smooth Mt. Fuji landscape with a global peak. Their model is exactly like ours, except they only consider the case that \(\delta=0\). Our work helps us quantify how in non-smooth landscapes (deaths represents unviable valleys or swamplands, or suboptimal local peaks) , the global peak may be found nonetheless, given a certain amount of local peaks and dead ends.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For a less speculative, more detailed treatment of these ideas, read the &lt;a href=&quot;https://arxiv.org/abs/1612.00825&quot;&gt;main paper&lt;/a&gt;. Comments, ideas to extend our work, and feedback is welcome.&lt;/p&gt;

&lt;h3 id=&quot;note-on-the-applicability-of-the-model&quot;&gt;Note on the applicability of the model&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Artem Kaznatcheev has written a great blog post on this work as well, with a slightly different lens, check it out &lt;a href=&quot;https://egtheory.wordpress.com/2016/12/18/fusion-and-sex/&quot;&gt;here&lt;/a&gt;. In particular, he points out that the benefits of sex, in the way we describe it, are compromised if there is a strict pairing between division/death events and merging events (namely, e.g. you have to split every time you merge). This is not necessary in protocells and viruses, but certainly relevant in other scenarios. (I will amend more notes here on applicability limitations when I have the chance).&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[1]Pugh W. 1990 Skip lists: a probabilistic alternative to balanced trees. Commun. ACM 33, 668 – 676. (doi:10.1145/78973.78977)&lt;/p&gt;

&lt;p&gt;[2]Wilf HS, Ewens WJ. 2010 There’s plenty of time for evolution. Proc. Natl Acad. Sci. USA 107&lt;/p&gt;

&lt;p&gt;[3]Luria, S. E. “Reactivation of irradiated bacteriophage by transfer of self-reproducing units.” Proceedings of the National Academy of Sciences 33.9 (1947): 253-264.&lt;/p&gt;

&lt;p&gt;[4]Kauffman, Stuart A. “Autocatalytic sets of proteins.” Journal of theoretical biology 119.1 (1986): 1-24.&lt;/p&gt;

&lt;p&gt;[5]Segré, Daniel, Dafna Ben-Eli, and Doron Lancet. “Compositional genomes: prebiotic information transfer in mutually catalytic noncovalent assemblies.” Proceedings of the National Academy of Sciences 97.8 (2000): 4112-4117.&lt;/p&gt;

&lt;p&gt;[6]Mann, Nicholas H., et al. “Marine ecosystems: bacterial photosynthesis genes in a virus.” Nature 424.6950 (2003): 741.&lt;/p&gt;</content><author><name>This work was done in collaboration with Jason Olejarz, Iulia Neagu and Martin Nowak.</name></author><summary type="html"></summary></entry><entry><title type="html">Using a Variational Auto-encoder to predict protein function</title><link href="http://localhost:4000/jekyll/update/2017/08/14/Using-a-Variational-Autoencoder-to-predict-protein-function.html" rel="alternate" type="text/html" title="Using a Variational Auto-encoder to predict protein function" /><published>2017-08-14T17:59:35-04:00</published><updated>2017-08-14T17:59:35-04:00</updated><id>http://localhost:4000/jekyll/update/2017/08/14/Using-a-Variational-Autoencoder-to-predict-protein-function</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2017/08/14/Using-a-Variational-Autoencoder-to-predict-protein-function.html">&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
        MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
          });

        MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
            for(i=0; i &lt; all.length; i += 1) {
                all[i].SourceElement().parentNode.className += ' has-jax';
            }
        });

&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; charset=&quot;utf-8&quot; src=&quot;https://vincenttam.github.io/javascripts/MathJaxLocal.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;NEWS&lt;/strong&gt;: This work was accepted as an oral presentation at &lt;a href=&quot;https://mlcb.github.io/&quot;&gt;NIPS MLCB workshop&lt;/a&gt;. See the &lt;a href=&quot;https://arxiv.org/pdf/1712.03346.pdf&quot;&gt;extended abstract&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this post I report an application of a Variational Auto-Encoder to predict the effects of mutations on protein function.  This post can serve as an “open notebook” for interested machine learning researchers to suggest improvements. If you think you have good ideas upon reading this, do not hesitate to contact me (you can find my contact information at the footer of this page).&lt;/p&gt;

&lt;h2 id=&quot;big-picture&quot;&gt;Big Picture&lt;/h2&gt;
&lt;p&gt;Proteins are responsible for the most diverse set of functions in biology. What proteins do is largely determined by their sequence (and conditions under which they fold). The ability to predict whether a change in sequence will affect their function is extremely valuable. There is a clear engineering/clinical incentive to modify protein function. For instance, my collaborators synthesize massive libraries of AAV variants, hoping to find effective viral vectors for gene therapy.&lt;/p&gt;

&lt;p&gt;My interest in this topic, as a computational biologist with interest in origin of life (and evolution), is driven more directly by basic science. Imagine you have a protein of size \(L\). The standard \(20\) amino-acid alphabet that could appear in any from position \(1\) to \(L\). This means that there are \(20^L\) possible sequences of size \(L\).  A key question in the field of origin of life, as well as evolutionary theory is this:&lt;/p&gt;

&lt;blockquote&gt;

 	 &lt;strong&gt;How many of these \(20^L\)  sequences result in a (specific) function?&lt;/strong&gt; 
&lt;/blockquote&gt;

&lt;p&gt;In other words, how dense is the sequence space in functional proteins? This space grows exponentially with sequence size, hence it is hopeless to sample it thoroughly even by computational means, let alone experiments. But navigating it in search for a function may also not be as bad as it initially seems, in particular, if functions are concentrated in clusters of (nearby) sequences. We can start from a known sequence that does have that function, and explore its vicinity.&lt;/p&gt;

&lt;p&gt;For the readers that are not very familiar with biology, maybe an analogy could be helpful. The English alphabet has 26+1 (for space) letters. For a sentence of length \(L\), there are \(27^L\) possible arrangments of these letters, most of them will have no meaning in English. However, a lot of sentences that do have a meaning are fairly close to each other. Hence, starting from a set of meaningful sentences, we can hope to train a model to reproduce them with some novelty. The problem for the protein engineer is significantly harder, because checking whether your algorithm produced a meaningful sequence requires you to actually make the protein (and deploy it in the right biological context) to evaluate its function. With sentences, verification is significantly easier and cheaper (a human can read and verify it). So the goal is to find a computational method where we can predict whether a sequence is functional (bio-active).&lt;/p&gt;

&lt;p&gt;In this blog post, I mention one approach I’ve recently tried with some success to predict protein functionality. I train a Variational Auto-encoder[1] on protein sequences that are close to each other (from an evolutionary perspective) and using that I try to predict how much the function of the protein is affected by changing one or two letters in a sequence. This is of course a much simpler problem than the big picture described above (but a first natural step to it). However, even when the problem is this much simpler, predicting protein function using current methods still has much room for improvement.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;A lot of recent success in machine learning, particularly in vision, is indebted to the vast amount of data that are compiled and accessible on the internet. Biologists have also become very good at generating massive amounts of data, especially sequence data from a variety of organisms. As a result, for many sequences of interest (be it DNA or protein sequences), one can search a huge database of sequences that are observed in other organisms, and align them to the sequence of interest (up to an arbitrarily constrained similarity threshold). This is called a &lt;strong&gt;multiple sequence alignment (MSA)&lt;/strong&gt;. An MSA constitutes an evolutionary cluster of nearby sequences. Note however, that these large datasets don’t generally come with “labels” (e.g. we don’t have a quantitative idea of how fast each protein metabolizes a compound). But because these sequences are observed in living organisms, we can safely assume that they are functional, and because of their similarity, they likely do similar things. Datasets that do provide quantitative measurements of function exist for some proteins (and we will make use of them, but not for training). But their size (and diversity) is generally more suitable for validation, rather than training. Hence, an unsupervised model seems to be a useful approach for this particular problem.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/alignment.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So what can we learn from alignment data? The toy model presented above can serve as an example. The alignment versus the reference sequence for 6 related sequences are shown. We can see for instance that columns (positions) 1 and 6 are fully conserved (independent of others). An “independent” model, learns the distribution of amino-acids in each column, and ascribes probabilities to each sequence accordingly. But there is more to learn from this data. If we start looking at pairwise interactions along the sequence, for instance, it appears that positions 3 and 10 have a pairwise interaction as D is always paired with Q, and W is always paired with A (this would be missed by an independent model). If our model is able to capture even higher order correlations, we can even say that it may be that position 5 also interacts with positions 3 and 10. In biology, mutations don’t necessarily affect the function independently (what is known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Epistasis&quot;&gt;epistasis&lt;/a&gt;). One mutation alone may be detrimental, but another one at a different position may “rescue” the protein to its original functionality.&lt;/p&gt;

&lt;p&gt;Based on this intuition, over the past few years generative models have been used on sequence alignments to predict protein structure and function [1],[2]. These models learn correlations between amino acids in different positions, and then try to guess (approximately) whether a change from one amino-acid to another in a given position(s) would be beneficial or detrimental for the function of the protein. The most successful applications of this approach have used &lt;a href=&quot;https://en.wikipedia.org/wiki/Potts_model&quot;&gt;Potts models&lt;/a&gt; as their core modeling approach. This approach models independent and pairwise interactions along the sequence. You can read the technical details &lt;a href=&quot;https://arxiv.org/pdf/1211.1281.pdf&quot;&gt;here&lt;/a&gt; and see its application for a large set of datasets &lt;a href=&quot;http://www.nature.com/nbt/journal/v35/n2/full/nbt.3769.html&quot;&gt;here&lt;/a&gt;. The methods show that harnessing correlations between pairs of amino acids at different positions provides significant power for protein folding and function prediction.&lt;/p&gt;

&lt;p&gt;In this post I show how to use a simple VAE [3] (another generative approximation method), to almost reproduce the results as predicted by the Potts model approach. Using auto-encoders for unsupervised protein fitness prediction has been a subject of interest for many groups. In fact, John Ingraham and Adam Reisselman from Marks lab have &lt;a href=&quot;https://drive.google.com/file/d/0ByOppqR-t6tRdGUzYjNOMEswUGM/view&quot;&gt;presented&lt;/a&gt; independently developed results on this at &lt;a href=&quot;https://www.broadinstitute.org/scientific-community/science/mia/models-inference-algorithms&quot;&gt;Broad MIA&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;I directly use the alignment data that accompanies Hopf, Ingraham, et al’s recent paper in Nature biotechnology and show that a simple VAE can perform closely to the performance of their model (in some cases with fewer parameters) in the single mutation and double mutation case. This method however should be seen as work in progress, as the VAE does not outperform the Potts model approach. The full workflow can be accessed in &lt;a href=&quot;https://github.com/samsinai/VAE_protein_function/blob/master/VAE_for_protein_function_prediction.ipynb&quot;&gt;this notebook&lt;/a&gt;. I highlight general results in this blog post.&lt;/p&gt;

&lt;p&gt;Before proceeding, I’d like to clarify two bits of terminology. First, in the entire workflow, we have one sequence that anchors our analysis. We run the alignment against that sequence (where we call it reference), and then we look at single, or double mutants of that sequence (these often don’t exists in the alignment, but we have experimental data for their function). In that context, I refer to the reference sequence as the “wildtype”. Second, fitness is a vague biological term. For our purposes, since what is being measured in each dataset may not be exactly the same thing, it is convenient to refer to it as fitness. As our interest is in relative performance (on whatever metric) between mutants, this is not unreasonable.&lt;/p&gt;

&lt;p&gt;To calculate a fitness value for particular sequence, I train a VAE with the following architecture on the alignment data:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;original_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ORDER_LIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PRUNED_SEQ_LENGTH&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ORDER_LIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PRUNED_SEQ_LENGTH&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;latent_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;intermediate_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;250&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nb_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;epsilon_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#np.random.seed(1337)  # for reproducibility
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sampling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;z_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z_log_var&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latent_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                              &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z_log_var&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;vae_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_decoded_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;xent_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;original_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;objectives&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;categorical_crossentropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;x_decoded_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kl_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z_log_var&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z_log_var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xent_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kl_loss&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#Encoding Layers
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;original_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intermediate_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;elu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intermediate_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'elu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNormalization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intermediate_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'elu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#Latent layers
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z_mean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latent_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;z_log_var&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latent_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Lambda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sampling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latent_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,))([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z_log_var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#Decoding layers 
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intermediate_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'elu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intermediate_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'elu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder_2d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder_3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intermediate_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'elu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoder_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sigmoid'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_decoded_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoder_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder_2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;vae&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_decoded_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#Potentially better results, but requires further hyperparameter tuning
#optimizer=keras.optimizers.SGD(lr=0.005, momentum=0.001, decay=0.0, nesterov=False,clipvalue=0.05)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vae&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;adam&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vae_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;categorical_accuracy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;fmeasure&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;top_k_categorical_accuracy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The hyperparameters for this model are manually optimized, but when using the ADAM optimizer, the results are generally not radically affected by reducing one or two layers, adding dropout, or adding more regularization. SGD is significantly more sensitive to these manipulations, but it is also thought to be potentially better at finding state of the art results. I have not managed to beat the results obtained by ADAM when using SGD.&lt;/p&gt;

&lt;p&gt;Once the network is trained, I use it to predict the probability of a mutant. To do so, I reconstruct the sequence it was fed, using its internal model (this is what an auto-encoder does).&lt;/p&gt;

&lt;p&gt;The one-hot encoding (input) of a sequence looks (almost) like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/doubles_input.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The reason I say almost is, that here I have actually superimposed two one hot encodings from two sequences. One is the wildtype sequence, and the other is the mutant. The white squares are shared between the two sequences. The colors show where the wildtype (red) and the mutant (yellow) differ in sequence, in this case in two positions. The one hot vector for each sequence is what is fed into the network. In the next two panels, I show the reconstruction of the two sequences by the trained network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/doubles_new_reconstruct.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These reconstructions are normalized like a probability weight matrix, where each position column constitutes probabilities for each amino-acid at that location (we denote this matrix as \(P\) below).&lt;/p&gt;

&lt;p&gt;The most interesting take away (and the reason I tried this) from these reconstructions is what the network has learned about co-dependence of mutations. To see this better, I subtract the the two PWMs and look at their difference below. The update in probability occurs not only in the position where the two sequences differ, but also in places that they are the same. The model is suggesting that maybe an additional mutation (for instance a “rescue”) in those places is now more likely (or less likely) given the mutation observed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/diff_reconstruct.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using these reconstructions, I compute the (log) probability of a sequence as:&lt;/p&gt;

\[\mathbb{P}(\sigma)=\log(trace(H^T P))\]

&lt;p&gt;Where \(H\) is the one-hot encoding of the sequence of interest, and \(P\) is the probability weight matrix generated by feeding the network a sequence. This formula allows me to calculate how probable a sequence is, given a probability weight matrix \(P\). I can compute \(P\) in three highly correlated ways (although depending on the dataset, these correlations change). The difference in these approaches is in how to compute \(P\) (you can read the details in the notebook). In this notebook, for simplicity, I use \(P\) as the reconstruction matrix that is obtained by feeding the network the input \(H\).&lt;/p&gt;

&lt;p&gt;Once we have computed \(\mathbb{P}(\sigma_i)\) for every sequence \(\sigma_i\) in our mutant set, we can compare the relative probability of each sequence with the experimentally measured function of the sequence. Below we show a scatter plot of the singles and double mutants for the PABP dataset. 
&lt;img src=&quot;http://localhost:4000/images/combined_single_double.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The red line denotes perfect rank correlation (spearman). The blue dots are the actual predictions vs. the experimental data. I can compare the performance of our model for both single and double mutants with that of Hopf. et al. In the table below, I show the spearman correlations between the experimental data, and that predicted by our method (approach 3), and compare it to the predictions done by the epistatic model (as well as the baseline independent model). I use the &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3851721/&quot;&gt;PABP yeast&lt;/a&gt; dataset[4] because it has both single and double mutant data.&lt;/p&gt;

&lt;p&gt;The results I report here are &lt;strong&gt;without&lt;/strong&gt; sequence reweighting. My references correct for oversampling (because MSA generates clusters of sequences), by dividing the weight of each sequence within a neighborhood by the size of its neighborhood.  My experience suggests that while tuning the reweighting parameter \(\theta\) does improve the predictions for each dataset, picking a pre-determined \(\theta\) such that \(0&amp;lt;\theta\leq 0.2\) across all datasets doesn’t consistently improve our predictions across datasets and on occasion deteriorates our results. The reader is welcome to use the reweighting logic provided in the notebook to attempt this by themselves.&lt;/p&gt;

&lt;style&gt;

table{
    vertical-align: center;
    border-collapse: collapse;
    border-spacing: 10px;
    padding:10px;
    border:2px solid #ff0000;

}

th{
    border:2px solid #000000;
    padding:4px;


}

td{
    border:1px solid #000000;
    padding:4px;

}
&lt;/style&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Method&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Singles&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Doubles&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Independent&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.42&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.49&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Epistatic (Hopf et al.)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.59&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.52&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;VAE&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.63&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.56&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I also run the same process on 5 other available datasets. Below I show the comparison between the VAE’s performance and that of the EVmutation (Hopf et al.). As it can be seen, our method mostly outperforms the independent model, meaning we are capturing higher-order interactions in the sequence. However, it also appears that our model occasionally misses out on learning some pairwise interactions. Note that the \(P\) matrix in this comparison is slightly more complicated (see notebook for details).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/meta_comp.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;under-the-hood-of-the-vae&quot;&gt;Under the hood of the VAE&lt;/h2&gt;

&lt;p&gt;One particularly appealing aspect of the VAE is that it performs a non-linear compression of the data into a set of latent variables. What I hope to learn is whether this compression (encoding) scheme learned by the network can provide biological insight. In particular whether walking in the lower dimensional manifold that the network projects onto includes biologically relevant (and interpretable) features.&lt;/p&gt;

&lt;p&gt;The training data (as projected in the latent domain) are presented in blue. The wildtype is indicated by a larger red dot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/full_latent.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is more information to be extracted from the latent space. I redraw this structure, but color the background by computing the \(\mathbb{P}(\sigma_{wt})\) at each location using the \(H_{wt}\) as input and \(P\) as the PWM induced by that point in the latent space. White areas are where wildtype occurs with the same probability as it’s own coordinate in the latent space. Red means that wildtype is less favored in those locations, and blue means it is more favored. To better understand the basis of this structure, I have now also color coded the training data using k-means clustering.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/combined_latent.jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is apparent that the clusters/branches in the latent dimension correspond to the min-distance clusters. In the zoomed in version near the wildtype (red dot), we can see the scatter of single (green) and double mutants (purple) where we have experimental data available. As it is clear, we only get to test the model on a very small region of the latent space. Ideally, our experimental data would also have sequences that are located in other parts of the latent space.&lt;/p&gt;

&lt;p&gt;To get a better sense of what is occuring in the sequence space along the axis of maximum variance (in the neighborhood of the sequence of interest) I use PCA to compute the principal eigenvector using only the test data in the latent space (compressing the entire training data to one dimension is more lossy). I then walk along this axis in the latent space, this is presented in the animation below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/images/walk.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Walking in the latent dimension results in a variety of sequences. On the left panel, I show the distribution of sequences induced by the point corresponding to the cyan point in the latent space (on the right panel). The most likely sequence is written atop the left panel.&lt;/p&gt;

&lt;p&gt;While these are interesting to observe, it remains unclear what we can learn from the structure of the latent space and the distributions that were learned for sequeneces in different coordinates of this space.&lt;/p&gt;

&lt;p&gt;In the following segment, I touch on this, as well as other areas of this approach where I think suggestions by people with more experience in deep learning would be helpful.&lt;/p&gt;

&lt;h3 id=&quot;questions-and-areas-for-potential-improvement&quot;&gt;Questions and areas for potential improvement&lt;/h3&gt;

&lt;p&gt;1- &lt;strong&gt;Hyper parameter optimization&lt;/strong&gt;: While I have done some hyperparameter optimization (number of layers, regularization, dropout layers, number of hidden units) by random checking, I am not sure if there are obvious improvements that I am missing out on. ADAM is fairly robust to these changes, however, SGD, which presumably could find better optima, is rather sensitive and blows up quickly. I have not achieved better results with SGD compared to ADAM.&lt;/p&gt;

&lt;p&gt;2-&lt;strong&gt;Network types&lt;/strong&gt;: I have experimented with a simple convolutional architecture, as well as recurrent layers in my setup, none of which come close to the performance of the dense layers. In principle I expect the recurrent layers to learn useful information about the protein, but this has not materialized. What are some sensible architectures to test?&lt;/p&gt;

&lt;p&gt;3-&lt;strong&gt;Interpretation of results&lt;/strong&gt;: I have provided some interpretations for what the network outputs, and explored the latent space.  However, it is clear that there is more lessons to be learned from the latent and hidden layers. What should I be looking for?&lt;/p&gt;

&lt;p&gt;4-&lt;strong&gt;Long training&lt;/strong&gt;: A lot of my runs, even on machines with large memory, result in memory errors mid training. I also get loss overflows (nan valued loss), when I use SGD with momentum (if I don’t clip the gradients aggressively). My guess is that this can be improved by hyperparameter tuning/regularization/constraining the network but how to correctly approach this is unclear to me.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This post serves as a proof of concept to demonstrate the ability of VAEs in capturing correlations in protein sequences. Interestingly, it works comparably to the “state of the art” methods that are based on Potts models. I am not the only one who has noticed this, and as mentioned before, Debora Marks’ lab has been working on a similar idea independently. In fact John Ingraham and Debora Marks have also written a nice paper on a variational inference as applicable to protein structure prediction[5]. However, I think that we are not currently using the full power of the model in practice. I hope that this post can help me and other interested researchers to improve on these results. Suggestions, ideas, and comments are welcome.&lt;/p&gt;

&lt;p&gt;References&lt;/p&gt;

&lt;p&gt;[1]: Ekeberg, Magnus, et al. “Improved contact prediction in proteins: using pseudolikelihoods to infer Potts models.” Physical Review E 87.1 (2013): 012707.&lt;/p&gt;

&lt;p&gt;[2]: Hopf, Thomas A., et al. “Mutation effects predicted from sequence co-variation.” Nature Biotechnology (2017).&lt;/p&gt;

&lt;p&gt;[3]: Kingma, Diederik P., and Max Welling. “Auto-encoding variational bayes.” arXiv preprint arXiv:1312.6114 (2013).&lt;/p&gt;

&lt;p&gt;[4]: Melamed, Daniel, et al. “Deep mutational scanning of an RRM domain of the Saccharomyces cerevisiae poly (A)-binding protein.” Rna 19.11 (2013): 1537-1551.&lt;/p&gt;

&lt;p&gt;[5]: Ingraham, John, and Debora Marks. “Variational Inference for Sparse and Undirected Models.” ICML 2017&lt;/p&gt;</content><author><name>This work was done in collaboration with Eric Kelsic. I'm also thankful to Pierce Ogden, Surojit Biswas, Gleb Kuznetsov, and Jeffery Gerold for helpful comments.</name></author><summary type="html"></summary></entry></feed>